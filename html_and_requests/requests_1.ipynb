{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python и интернет. Модуль requests\n",
    "\n",
    "**План**:\n",
    "\n",
    "1. Запросы\n",
    "2. Библиотеки для работы в питоне\n",
    "3. Сначала urllib + re briefly\n",
    "4. Потом requests + bs4 и почему так лучше\n",
    "5. Задание на семинар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как выкачать интернет\n",
    "Современный Интернет предоставляет лингвистам большое количество языковых данных: электронные газеты и журналы, блоги, форумы, социальные сети и т.д. Например, можно найти в сети много-много текстов и собрать корпус, или найти все газетные статьи и блог-посты про какую-нибудь корпорацию и проанализировать тональность сообщений. Сейчас мы научимся заниматься выкачиванием страниц из интернета с помощью Python.\n",
    "\n",
    "Для скачивания HTML-страниц в питоне есть несколько библиотек **urllib.request** и **requests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не забудьте сначала установить\n",
    "! pip install requests\n",
    "! pip install urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем сначала заюзать urllib.request\n",
    "### (Спойлер: модуль requests вообще-то круче)\n",
    "Допустим, мы хотим скачать главную страницу Хабрахабра.\n",
    "\n",
    "На самом деле, когда мы хотим открыть какую-то страницу в интернете, наш браузер отправляет на сервер **запрос** (\"Привет, сервер! я хочу код страницы по вот такому адресу!\"), а сервер затем отправляет ответ (\"Привет! Вот код страницы: ...\").\n",
    "Чтобы получить страницу через питон, нужно сформировать **запрос** на сервер так же, как это делает браузер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  # импортируем модуль \n",
    "\n",
    "req = urllib.request.Request('https://habr.com/ru/')\n",
    "with urllib.request.urlopen(req) as response:\n",
    "   html = response.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В переменной **req** у нас как раз находится запрос.\n",
    "Функция **urlopen** получает ответ сервера и скачивает страницу по ссылке https://habrahabr.ru/ в переменную **response**. **response** ведет себя как файл: например мы можем прочитать его содержимое с помощью **.read()** в другую переменную. \n",
    "Вот так просто мы сохранили код страницы в переменной **html**. Убедимся, что в там лежит html-код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"ru\" class=\"no-js\">\n",
      "  <head>\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta content='width=1024' name='viewport'>\n",
      "<title>Лучшие публикации за сутки / \n"
     ]
    }
   ],
   "source": [
    "print(html[:210])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда сайт блокирует запросы, если их посылает не настоящий браузер с пользователем, а какой-то бот (например, так делает Гугл или Википедия). Иногда сайты присылают разные версии страниц, разным браузерам.  \n",
    "По этим причинам полезно бывает писать скрипт, который умеет притворяться то одним, то другим браузером.\n",
    "Когда мы пытаемся получить страницу с помощью **urllib**, наш код по умолчанию честно сообщает серверу, что он является программой на питоне. Он говорит что-то вроде \"Привет, я Python-urllib/3.5\". \n",
    "Но можно, например, представиться Мозиллой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://habr.com/ru/'  # адрес страницы, которую мы хотим скачать\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'  # хотим притворяться браузером\n",
    "\n",
    "req = urllib.request.Request(url, headers={'User-Agent':user_agent})  \n",
    "# добавили в запрос информацию о том, что мы браузер Мозилла\n",
    "\n",
    "with urllib.request.urlopen(req) as response:\n",
    "   html = response.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А можно еще так\n",
    "from fake_useragent import UserAgent\n",
    "user_agent = UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"ru\" class=\"no-js\">\n",
      "  <head>\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta content='width=1024' name='viewport'>\n",
      "<title>Лучшие публикации за сутки / Хабр</title>\n",
      "\n",
      "  <meta name=\"description\" content=\"Лучшие публикации за последние 24 часа\" \n"
     ]
    }
   ],
   "source": [
    "print(html[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура, всё на месте!\n",
    "\n",
    "Но что всё это значит? Что такое html и как вообще из него доставать какую-то информацию?\n",
    "\n",
    "Ответ: по **тегам**! Например, в куске html сверху есть теги `<title> </title>` (теги всегда обрамляют с двух сторон то, что находится под этим тегом). В `<title>` в данном случае лежит заголовок этой интернет-страницы.\n",
    "\n",
    "Существует несколько вариантов, как достать что-то из определенного тега, например, достать заголовок:\n",
    "\n",
    "1. регулярные выражения\n",
    "2. специальные библиотеки питона, например, BeautifulSoup (bs4) или lxml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что мы хотим выкачивать заголовки статей с главной страницы Хабрахабра. Код страницы у нас уже есть, но как из него что-то вытащить. Для начала нужно посмотреть в исходник (view-source:https://habr.com/ru/) и заметить, что заголовки хранятся в тэге **h2** с классом **post__title**. Заголовок выглядит примерно так:\n",
    "\n",
    "<h2 class=\"post__title\">\n",
    "    <a href=\"https://habr.com/ru/company/recognitor/blog/474674/\" class=\"post__title_link\">Машинное зрение и медицина</a>\n",
    "  </h2>\n",
    "  \n",
    "Видите, маркдаун понимает html код\n",
    "\n",
    "А вот и сам html код (это запускать нельзя и не получится, потому что это код \"языка\" html): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2 class=\"post__title\">\n",
    "    <a href=\"https://habr.com/ru/company/recognitor/blog/474674/\" class=\"post__title_link\">Машинное зрение и медицина</a>\n",
    "  </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы вытащить все такие заголовки постов, воспользуемся регулярным выражением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regPostTitle = re.compile('<h2 class=\"post__title\">(.*?)</h2>', flags= re.DOTALL)\n",
    "titles = regPostTitle.findall(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько там заголовков. И взглянем, например, на первые три."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n    <a href=\"https://habr.com/ru/post/474816/\" class=\"post__title_link\">Pet-проекты — маленькая жизнь</a>\\n  ', '\\n    <a href=\"https://habr.com/ru/company/croc/blog/474790/\" class=\"post__title_link\">Байки переговорщика</a>\\n  ', '\\n    <a href=\"https://habr.com/ru/company/yandex/blog/474438/\" class=\"post__title_link\">Просто и на C++. Основы Userver — фреймворка для написания асинхронных микросервисов</a>\\n  ']\n"
     ]
    }
   ],
   "source": [
    "print(titles[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте очистим заголовки от лишних переносов строк, лишних тэгов и распечатаем их подряд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pet-проекты — маленькая жизнь\n",
      "Байки переговорщика\n",
      "Просто и на C++. Основы Userver — фреймворка для написания асинхронных микросервисов\n",
      "Как работает криптография на основе эллиптических кривых в TLS 1.3\n",
      "«Шакал»: сжимаем фронтенд\n",
      "Древности: вестник тлена или незапланированное устаревание\n",
      "Обзор Skaffold для разработки под Kubernetes\n",
      "Машинное зрение и медицина\n",
      "Как добавить проверки в NoVerify, не написав ни строчки Go-кода\n",
      "LEGO MINDSTORMS Education EV3 + MicroPython: программируем детский конструктор взрослым языком\n",
      "Выпуск Rust 1.39.0: async/await, аттрибуты для параметров функций, новые константные функции\n",
      "Обзор накопителя Seagate ST2000DM008: быстрая «двушка» без оверпрайса\n",
      "«Извини, но у меня депрессия»: как работать с заболевшим сотрудником\n",
      "Типы для HTTP-API, написанных на Python: опыт Instagram\n",
      "Кейс: Автосервис. Разработка рекламных кампаний и внедрение Битрикс24\n",
      "Учеными обнаружен новый фактор эффективной доставки лекарств в опухоли\n",
      "Performance in .NET Core\n",
      "Представлена программа очистки мировых рек от мусора\n",
      "Видеозвонки, WebRTC и браузер: как это работает и как согреть «замерзающую» трансляцию\n"
     ]
    }
   ],
   "source": [
    "new_titles = []\n",
    "regTag = re.compile('<.*?>', re.DOTALL)\n",
    "regSpace = re.compile('\\s{2,}', re.DOTALL)\n",
    "for t in titles:\n",
    "    clean_t = regSpace.sub(\"\", t)\n",
    "    clean_t = regTag.sub(\"\", clean_t)\n",
    "    new_titles.append(clean_t)\n",
    "for t in new_titles:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и осталось убрать некрасивые кусочки html, а именно заменить специальные html-последовательности nbsp и rarr на стрелочку, например."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pet-проекты — маленькая жизнь\n",
      "Байки переговорщика\n",
      "Просто и на C++. Основы Userver — фреймворка для написания асинхронных микросервисов\n",
      "Как работает криптография на основе эллиптических кривых в TLS 1.3\n",
      "«Шакал»: сжимаем фронтенд\n",
      "Древности: вестник тлена или незапланированное устаревание\n",
      "Обзор Skaffold для разработки под Kubernetes\n",
      "Машинное зрение и медицина\n",
      "Как добавить проверки в NoVerify, не написав ни строчки Go-кода\n",
      "LEGO MINDSTORMS Education EV3 + MicroPython: программируем детский конструктор взрослым языком\n",
      "Выпуск Rust 1.39.0: async/await, аттрибуты для параметров функций, новые константные функции\n",
      "Обзор накопителя Seagate ST2000DM008: быстрая «двушка» без оверпрайса\n",
      "«Извини, но у меня депрессия»: как работать с заболевшим сотрудником\n",
      "Типы для HTTP-API, написанных на Python: опыт Instagram\n",
      "Кейс: Автосервис. Разработка рекламных кампаний и внедрение Битрикс24\n",
      "Учеными обнаружен новый фактор эффективной доставки лекарств в опухоли\n",
      "Performance in .NET Core\n",
      "Представлена программа очистки мировых рек от мусора\n",
      "Видеозвонки, WebRTC и браузер: как это работает и как согреть «замерзающую» трансляцию\n"
     ]
    }
   ],
   "source": [
    "for t in new_titles:\n",
    "    print(t.replace(\"&nbsp;&rarr;\", \" -> \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Некоторые объяснения про регулярные выражения (Лирическое отступление)\n",
    "\n",
    "* Что такое `re.compile`? <br><br>\n",
    "Грубо говоря, `compile()` позволяет запомнить регулярное выражение и использовать его несколько раз. Суть в том, что перед тем как прогнать регулярку через строку, питон должен ее \"скомпилировать\" - превратить **строку** с регулярным выражением в специальный **объект**.<br>\n",
    "Строчка `re.search(..., ...)` сначала компилирует регулярное выражение, а потом выполняет поиск. Если нужно поискать что-то один раз, то такая строчка очень удобна. А если нужно поискать что-то много раз, то получится что одно и то же выражение мы компилируем много раз. А хочется один раз скомпилировать и потом много раз пользоваться. Поэтому пишут так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'тут текст, внутри которого мы что-то ищем'\n",
    "regName = re.compile('тут регулярное выражение') # скомпилировали\n",
    "toSearch = regName.search(text) # теперь можно искать в тексте\n",
    "toFindAll = regName.findall(text)  # можно использовать скомпилированное выражение много раз\n",
    "toSub = regName.sub('на.что.заменить', text) # и так тоже можно использовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Что делает `regName.sub(..., ...)`?<br><br>\n",
    "Выражение  `regName.sub('на_что_заменить', text)` значит: возьми скомпилированное выражение из переменной `regName`, и замени все, что соответствует этому выражению в переменной `text`, на строку `'на_что_заменить'`. Если первый аргумент в этом случае - пустая строка, то все найденные регуляркой куски заменятся на пустую строку, короче говоря, удалятся.<br><br>\n",
    "\n",
    "* Что такое `re.DOTALL`?<br><br>\n",
    "Обычно точка в регулярном выражении означает любой символ КРОМЕ символа новой строки.  Чтобы изменить такое поведение, в компиляцию регулярки можно добавить параметры-флаги вот так: `flags = re.DOTALL`, и тогда точка будет ловить вообще любой символ, включая новую строку. Эти флаги слегка меняют поведение функции, вот и все.<br><br>\n",
    "\n",
    "* Что такое `re.U`?<br><br>\n",
    "Про эту штуку нужно знать, если вы работаете с регулярками на втором питоне. Дело в том, что во втором питоне по умолчанию выражения типа `\\w`, `\\W`, `\\s` и подобные работают только на строках ASCII, и чтобы они работали на юникодных строках нужно поставить флаг re.U. В третьем питоне все строки и так юникодные, поэтому необходимости в таком флаге нет.\n",
    "\n",
    "# Но\n",
    "## Requests + bs4 в разы круче всего этого выше\n",
    "\n",
    "Давайте разберёмся, почему. Во-первых, requests делает тоже самое за гораздо меньше символов кода. Во-вторых у него есть куча всяких других [плюшек](https://realpython.com/python-requests/). Мы разберем некоторые из них.\n",
    "\n",
    "Сначала пошлём запрос get к странице тоже хабрахабра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get('https://habr.com/ru')\n",
    "html = result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"ru\" class=\"no-js\">\n",
      "  <head>\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta content='width=1024' name='viewport'>\n",
      "<title>Лучшие публикации за сутки / Хабр</title>\n",
      "\n",
      "  <meta name=\"description\" content=\"Лучшие публикации за последние 24 часа\" \n"
     ]
    }
   ],
   "source": [
    "print(html[:300])  # То же самое, но в разы короче!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, что и urllib, но записано в разы короче! \n",
    "\n",
    "Ещё с помощью requests можно проверять существует ли страница, к которой вы посылаете запрос, не заглядывая в html. (Если 200, то всё ок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://habr.com/ru').status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили точно такую же переменную с html, как и до этого с помощью метода .get.text. Теперь попробуем снова достать оттуда те же самые заголовки. Но теперь используем для этой задачи **BeautifulSoup** (Этот модуль назван в честь стишка про прекрасный суп из Алисы в стране чудес, но для меня ирония отсалась непонятной)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала инициализируем объект BeautifulSoup. Потом применим метод find и в скобочках укажем теги, по которым ищем. У некоторых тегов в html (как и в нашем случае) бывает еще и class и какие-нибудь еще атрибуты. Такие вещи мы задаем словариком.\n",
    "\n",
    "Этот запрос вернёт нам только первый заголовок. То есть первое вхождение такого тега в нашем html файле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pet-проекты — маленькая жизнь\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/post/474816/\">\n",
      "  Pet-проекты — маленькая жизнь\n",
      " </a>\n",
      "</h2>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html,'html.parser')  # инициализируем суп\n",
    "\n",
    "post = soup.find('h2', {'class': 'post__title'})\n",
    "print(post.get_text())\n",
    "print(post.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы хотим получить все заголовки постов! Метод find_all возвращает массив всех элементов с тегом указанным в скобках. По нему можно итерироваться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pet-проекты — маленькая жизнь\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/post/474816/\">\n",
      "  Pet-проекты — маленькая жизнь\n",
      " </a>\n",
      "</h2>\n",
      "\n",
      "-- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "Байки переговорщика\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/company/croc/blog/474790/\">\n",
      "  Байки переговорщика\n",
      " </a>\n",
      "</h2>\n",
      "\n",
      "-- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "Просто и на C++. Основы Userver — фреймворка для написания асинхронных микросервисов\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/company/yandex/blog/474438/\">\n",
      "  Просто и на C++. Основы Userver — фреймворка для написания асинхронных микросервисов\n",
      " </a>\n",
      "</h2>\n",
      "\n",
      "-- -- -- -- -- -- -- -- -- -- \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html,'html.parser')  # инициализируем суп\n",
    "\n",
    "for post in soup.find_all('h2', {'class': 'post__title'})[:3]:\n",
    "    print(post.get_text())\n",
    "    print(post.prettify())\n",
    "\n",
    "    print('-- '*10)  # для красоты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на семинар 1\n",
    "\n",
    "А что если мы хотим зайти еще глубже по дереву тегов и, например, для каждого заголовка поста найти никнейм юзера, который написал этот пост, и время написания поста?\n",
    "\n",
    "Для этого надо снова зайти в просмотор кода страницы и увидеть, что там просиходит что-то такое:\n",
    "\n",
    "(Заодно обратите внимание, как пишутся комменты в html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<li class=\"content-list__item content-list__item_post shortcuts_item\" id=\"post_474790\">  <!--У каждого поста есть номер-->\n",
    "                \n",
    "    <article class=\"post post_preview\" lang=\"ru\">\n",
    "  <header class=\"post__meta\">\n",
    "    <a href=\"https://habr.com/ru/users/SStrelkov/\" class=\"post__user-info user-info\" title=\"Автор публикации\">\n",
    "        <img src=\"//habrastorage.org/getpro/habr/avatars/62a/d56/1fd/62ad561fd1e297066e0ccdcce8001192.jpg\" width=\"24\" height=\"24\" class=\"user-info__image-pic user-info__image-pic_small\">\n",
    "      <span class=\"user-info__nickname user-info__nickname_small\">SStrelkov</span>  <!--А вот и никнейм-->\n",
    "    </a>                            <!--Никнейм лежит глубже номера, в header, в span class user info...-->\n",
    "\n",
    "    <span class=\"post__time\">сегодня в 10:03</span>  <!--А вот и время, тоже в header, но у span другой класс-->\n",
    "\n",
    "    \n",
    "  </header>\n",
    "\n",
    "  <h2 class=\"post__title\">\n",
    "    <a href=\"https://habr.com/ru/company/croc/blog/474790/\" class=\"post__title_link\">Байки переговорщика</a>\n",
    "  </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать словарик, где ключом будет id поста, а значением словарь или кортеж из трех переменных: название поста, никнейм автора и время написания для всех постов с главной страницы хабра, на которую мы смотрим весь семинар."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на семинар 2\n",
    "\n",
    "1. Скачать главную страницу Яндекс.Погоды и <br>\n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;а) распечатать сегодняшнюю температуру и облачность<br>\n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;б) распечатать время восхода и заката<br>\n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;в) погоду на завтра<br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
