{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python и интернет. Модуль requests\n",
    "\n",
    "**План**:\n",
    "\n",
    "1. Запросы\n",
    "2. Библиотеки для работы в питоне\n",
    "3. Сначала urllib + re briefly\n",
    "4. Потом requests + bs4 и почему так лучше\n",
    "5. Задание на семинар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как выкачать интернет\n",
    "Современный Интернет предоставляет лингвистам большое количество языковых данных: электронные газеты и журналы, блоги, форумы, социальные сети и т.д. Например, можно найти в сети много-много текстов и собрать корпус, или найти все газетные статьи и блог-посты про какую-нибудь корпорацию и проанализировать тональность сообщений. Сейчас мы научимся заниматься выкачиванием страниц из интернета с помощью Python.\n",
    "\n",
    "Для скачивания HTML-страниц в питоне есть несколько библиотек **urllib.request** и **requests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не забудьте сначала установить\n",
    "! pip install requests\n",
    "! pip install urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем сначала заюзать urllib.request\n",
    "### (Спойлер: модуль requests вообще-то круче)\n",
    "Допустим, мы хотим скачать главную страницу Хабрахабра.\n",
    "\n",
    "На самом деле, когда мы хотим открыть какую-то страницу в интернете, наш браузер отправляет на сервер **запрос** (\"Привет, сервер! я хочу код страницы по вот такому адресу!\"), а сервер затем отправляет ответ (\"Привет! Вот код страницы: ...\").\n",
    "Чтобы получить страницу через питон, нужно сформировать **запрос** на сервер так же, как это делает браузер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  # импортируем модуль \n",
    "\n",
    "req = urllib.request.Request('https://habr.com/ru/') # формируем запрос\n",
    "\n",
    "response = urllib.request.urlopen(req) # открываем страничку по запросу\n",
    "\n",
    "html_content = response.read().decode('utf-8') # читаем ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В переменной **req** у нас как раз находится запрос.\n",
    "Функция **urlopen** получает ответ сервера и скачивает страницу по ссылке https://habrahabr.ru/ в переменную **response**. **response** ведет себя как файл: например мы можем прочитать его содержимое с помощью **.read()** в другую переменную. \n",
    "Вот так просто мы сохранили код страницы в переменной **html**. Убедимся, что в там лежит html-код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"ru\" class=\"no-js\">\n",
      "  <head>\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta content='width=1024' name='viewport'>\n",
      "<title>Лучшие публикации за сутки / \n"
     ]
    }
   ],
   "source": [
    "print(html_content[:210])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда сайт блокирует запросы, если их посылает не настоящий браузер с пользователем, а какой-то бот (например, так делает Гугл или Википедия). Иногда сайты присылают разные версии страниц, разным браузерам.  \n",
    "По этим причинам полезно бывает писать скрипт, который умеет притворяться то одним, то другим браузером.\n",
    "Когда мы пытаемся получить страницу с помощью **urllib**, наш код по умолчанию честно сообщает серверу, что он является программой на питоне. Он говорит что-то вроде \"Привет, я Python-urllib/3.5\". \n",
    "Но можно, например, представиться Мозиллой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://habr.com/ru/'  # адрес страницы, которую мы хотим скачать\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'  # хотим притворяться браузером\n",
    "\n",
    "req = urllib.request.Request(url, headers={'User-Agent':user_agent})  \n",
    "# добавили в запрос информацию о том, что мы браузер Мозилла\n",
    "\n",
    "with urllib.request.urlopen(req) as response:\n",
    "    html_content = response.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А можно еще так\n",
    "from fake_useragent import UserAgent\n",
    "user_agent = UserAgent().chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"ru\" class=\"no-js\">\n",
      "  <head>\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta content='width=1024' name='viewport'>\n",
      "<title>Лучшие публикации за сутки / Хабр</title>\n",
      "\n",
      "  <meta name=\"description\" content=\"Лучшие публикации за последние 24 часа\" \n"
     ]
    }
   ],
   "source": [
    "print(html_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура, всё на месте!\n",
    "\n",
    "Но что всё это значит? Что такое html и как вообще из него доставать какую-то информацию?\n",
    "\n",
    "Ответ: по **тегам**! Например, в куске html сверху есть теги `<title> </title>` (теги всегда обрамляют с двух сторон то, что находится под этим тегом). В `<title>` в данном случае лежит заголовок этой интернет-страницы.\n",
    "\n",
    "Существует несколько вариантов, как достать что-то из определенного тега, например, достать заголовок:\n",
    "\n",
    "1. регулярные выражения\n",
    "2. специальные библиотеки питона, например, BeautifulSoup (bs4) или lxml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что мы хотим выкачивать заголовки статей с главной страницы Хабрахабра. Код страницы у нас уже есть, но как из него что-то вытащить. Для начала нужно посмотреть в исходник (view-source:https://habr.com/ru/) и заметить, что заголовки хранятся в тэге **h2** с классом **post__title**. Заголовок выглядит примерно так:\n",
    "\n",
    "<h2 class=\"post__title\">\n",
    "    <a href=\"https://habr.com/ru/company/recognitor/blog/474674/\" class=\"post__title_link\">Машинное зрение и медицина</a>\n",
    "  </h2>\n",
    "  \n",
    "Видите, маркдаун понимает html код\n",
    "\n",
    "А вот и сам html код (это запускать нельзя и не получится, потому что это код \"языка\" html): "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<h2 class=\"post__title\">\n",
    "    <a href=\"https://habr.com/ru/company/recognitor/blog/474674/\" class=\"post__title_link\">Машинное зрение и медицина</a>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы вытащить все такие заголовки постов, воспользуемся регулярным выражением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_post_title = re.compile('<h2 class=\"post__title\">(.*?)</h2>', flags= re.DOTALL)\n",
    "titles = regex_post_title.findall(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько там заголовков. И взглянем, например, на первые три."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n    <a href=\"https://habr.com/ru/post/475146/\" class=\"post__title_link\">Хождение по мукам или долгая история одной попытки восстановления данных</a>\\n  ', '\\n    <a href=\"https://habr.com/ru/post/475152/\" class=\"post__title_link\">Лицемерие google. PageSpeed Insights</a>\\n  ', '\\n    <a href=\"https://habr.com/ru/post/475138/\" class=\"post__title_link\">5 способов полезного использования Raspberry Pi. Часть вторая</a>\\n  ']\n"
     ]
    }
   ],
   "source": [
    "print(titles[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте очистим заголовки от лишних переносов строк, лишних тэгов и распечатаем их подряд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Хождение по мукам или долгая история одной попытки восстановления данных\n",
      "Лицемерие google. PageSpeed Insights\n",
      "5 способов полезного использования Raspberry Pi. Часть вторая\n",
      "Большое интервью про Big Data: зачем за нами следят в соцсетях и кто продает наши данные?\n",
      "Китайский ветряк, часть 2 заключительная\n",
      "Сказ об опасном std::enable_shared_from_this, или антипаттерн «Зомби»\n",
      "Как взлететь на батарейках или немного теории электропарамотора. Часть 1\n",
      "Как взлететь на батарейках или практика эксплуатации электропарамотора SkyMax. Часть 2\n",
      "Сон, релаксация и музыка: как профессиональные атлеты преодолевают усталость, и что нам с этого\n",
      "Делим Laravel на компоненты\n",
      "Как не переписать проект на Rust\n",
      "Обсуждение: работа интернета держится на open source — какие аргументы есть у критиков\n",
      "Замена EAV на JSONB в PostgreSQL\n",
      "Низкорисковые биржевые инвестиции: как использовать счета ИИС и облигации как альтернативу банковским вкладам\n",
      "Таймлапс собственными силами с облачного сервиса видеонаблюдения IPEYE\n",
      "Применение X-Macro в модерновом C++ коде\n",
      "Проблемы основных паттернов создания data-driven apps на React.JS\n",
      "Функциональное программирование с точки зрения EcmaScript. Чистые функции, лямбды, имутабельность\n",
      "Лексическое окружение (LexicalEnvironment) и Замыкание (Closures) в EcmaScript\n"
     ]
    }
   ],
   "source": [
    "new_titles = []\n",
    "regex_tag = re.compile('<.*?>', re.DOTALL)\n",
    "regex_space = re.compile('\\s{2,}', re.DOTALL)\n",
    "for t in titles:\n",
    "    clean_t = regex_space.sub(\"\", t)\n",
    "    clean_t = regex_tag.sub(\"\", clean_t)\n",
    "    new_titles.append(clean_t)\n",
    "for t in new_titles:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и осталось убрать некрасивые кусочки html, а именно заменить специальные html-последовательности nbsp и rarr на стрелочку, например."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Хождение по мукам или долгая история одной попытки восстановления данных\n",
      "Лицемерие google. PageSpeed Insights\n",
      "5 способов полезного использования Raspberry Pi. Часть вторая\n",
      "Большое интервью про Big Data: зачем за нами следят в соцсетях и кто продает наши данные?\n",
      "Китайский ветряк, часть 2 заключительная\n",
      "Сказ об опасном std::enable_shared_from_this, или антипаттерн «Зомби»\n",
      "Как взлететь на батарейках или немного теории электропарамотора. Часть 1\n",
      "Как взлететь на батарейках или практика эксплуатации электропарамотора SkyMax. Часть 2\n",
      "Сон, релаксация и музыка: как профессиональные атлеты преодолевают усталость, и что нам с этого\n",
      "Делим Laravel на компоненты\n",
      "Как не переписать проект на Rust\n",
      "Обсуждение: работа интернета держится на open source — какие аргументы есть у критиков\n",
      "Замена EAV на JSONB в PostgreSQL\n",
      "Низкорисковые биржевые инвестиции: как использовать счета ИИС и облигации как альтернативу банковским вкладам\n",
      "Таймлапс собственными силами с облачного сервиса видеонаблюдения IPEYE\n",
      "Применение X-Macro в модерновом C++ коде\n",
      "Проблемы основных паттернов создания data-driven apps на React.JS\n",
      "Функциональное программирование с точки зрения EcmaScript. Чистые функции, лямбды, имутабельность\n",
      "Лексическое окружение (LexicalEnvironment) и Замыкание (Closures) в EcmaScript\n"
     ]
    }
   ],
   "source": [
    "for t in new_titles:\n",
    "    print(t.replace(\"&nbsp;&rarr;\", \" -> \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Но! Мы не знаем, что у нас попадется, но это все можно сделать автоматически с помощью html unescape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "текст1 текст2\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "print(html.unescape('текст1&nbsp;текст2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Некоторые объяснения про регулярные выражения (Лирическое отступление)\n",
    "\n",
    "* Что такое `re.compile`? <br><br>\n",
    "Грубо говоря, `compile()` позволяет запомнить регулярное выражение и использовать его несколько раз. Суть в том, что перед тем как прогнать регулярку через строку, питон должен ее \"скомпилировать\" - превратить **строку** с регулярным выражением в специальный **объект**.<br>\n",
    "Строчка `re.search(..., ...)` сначала компилирует регулярное выражение, а потом выполняет поиск. Если нужно поискать что-то один раз, то такая строчка очень удобна. А если нужно поискать что-то много раз, то получится что одно и то же выражение мы компилируем много раз. А хочется один раз скомпилировать и потом много раз пользоваться. Поэтому пишут так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'тут текст, внутри которого мы что-то ищем'\n",
    "regex_name = re.compile('тут регулярное выражение') # скомпилировали\n",
    "to_search = regex_name.search(text) # теперь можно искать в тексте\n",
    "to_findall = regex_name.findall(text)  # можно использовать скомпилированное выражение много раз\n",
    "to_sub = regex_name.sub('на.что.заменить', text) # и так тоже можно использовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Что делает `reg_name.sub(..., ...)`?<br><br>\n",
    "Выражение  `reg_name.sub('на_что_заменить', text)` значит: возьми скомпилированное выражение из переменной `reg_name`, и замени все, что соответствует этому выражению в переменной `text`, на строку `'на_что_заменить'`. Если первый аргумент в этом случае - пустая строка, то все найденные регуляркой куски заменятся на пустую строку, короче говоря, удалятся.<br><br>\n",
    "\n",
    "* Что такое `re.DOTALL`?<br><br>\n",
    "Обычно точка в регулярном выражении означает любой символ КРОМЕ символа новой строки.  Чтобы изменить такое поведение, в компиляцию регулярки можно добавить параметры-флаги вот так: `flags = re.DOTALL`, и тогда точка будет ловить вообще любой символ, включая новую строку. Эти флаги слегка меняют поведение функции, вот и все.<br><br>\n",
    "\n",
    "* Что такое `re.U`?<br><br>\n",
    "Про эту штуку нужно знать, если вы работаете с регулярками на втором питоне. Дело в том, что во втором питоне по умолчанию выражения типа `\\w`, `\\W`, `\\s` и подобные работают только на строках ASCII, и чтобы они работали на юникодных строках нужно поставить флаг re.U. В третьем питоне все строки и так юникодные, поэтому необходимости в таком флаге нет.\n",
    "\n",
    "# Но\n",
    "## Requests + bs4 в разы круче всего этого выше\n",
    "\n",
    "Давайте разберёмся, почему. Во-первых, requests делает тоже самое за гораздо меньше символов кода. Во-вторых у него есть куча всяких других [плюшек](https://realpython.com/python-requests/). Мы разберем некоторые из них.\n",
    "\n",
    "Сначала пошлём запрос get к странице тоже хабрахабра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get('https://habr.com/ru')\n",
    "html = result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"ru\" class=\"no-js\">\n",
      "  <head>\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta content='width=1024' name='viewport'>\n",
      "<title>Лучшие публикации за сутки / Хабр</title>\n",
      "\n",
      "  <meta name=\"description\" content=\"Лучшие публикации за последние 24 часа\" \n"
     ]
    }
   ],
   "source": [
    "print(html[:300])  # То же самое, но в разы короче!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, что и urllib, но записано в разы короче! \n",
    "\n",
    "Ещё с помощью requests можно проверять существует ли страница, к которой вы посылаете запрос, не заглядывая в html. (Если 200, то всё ок)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://habr.com/ru').status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили точно такую же переменную с html, как и до этого с помощью метода .get.text. Теперь попробуем снова достать оттуда те же самые заголовки. Но теперь используем для этой задачи **BeautifulSoup** (Этот модуль назван в честь стишка про прекрасный суп из Алисы в стране чудес, но для меня ирония отсалась непонятной)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала инициализируем объект BeautifulSoup. Потом применим метод find и в скобочках укажем теги, по которым ищем. У некоторых тегов в html (как и в нашем случае) бывает еще и class и какие-нибудь еще атрибуты. Такие вещи мы задаем словариком.\n",
    "\n",
    "Этот запрос вернёт нам только первый заголовок. То есть первое вхождение такого тега в нашем html файле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Хождение по мукам или долгая история одной попытки восстановления данных\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/post/475146/\">\n",
      "  Хождение по мукам или долгая история одной попытки восстановления данных\n",
      " </a>\n",
      "</h2>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html,'html.parser')  # инициализируем суп\n",
    "\n",
    "post = soup.find('h2', {'class': 'post__title'})\n",
    "print(post.get_text())\n",
    "print(post.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы хотим получить все заголовки постов! Метод find_all возвращает массив всех элементов с тегом указанным в скобках. По нему можно итерироваться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Хождение по мукам или долгая история одной попытки восстановления данных\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/post/475146/\">\n",
      "  Хождение по мукам или долгая история одной попытки восстановления данных\n",
      " </a>\n",
      "</h2>\n",
      "\n",
      "-- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "Лицемерие google. PageSpeed Insights\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/post/475152/\">\n",
      "  Лицемерие google. PageSpeed Insights\n",
      " </a>\n",
      "</h2>\n",
      "\n",
      "-- -- -- -- -- -- -- -- -- -- \n",
      "\n",
      "5 способов полезного использования Raspberry Pi. Часть вторая\n",
      "\n",
      "<h2 class=\"post__title\">\n",
      " <a class=\"post__title_link\" href=\"https://habr.com/ru/post/475138/\">\n",
      "  5 способов полезного использования Raspberry Pi. Часть вторая\n",
      " </a>\n",
      "</h2>\n",
      "\n",
      "-- -- -- -- -- -- -- -- -- -- \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html,'html.parser')  # инициализируем суп\n",
    "\n",
    "for post in soup.find_all('h2', {'class': 'post__title'})[:3]:\n",
    "    print(post.get_text())\n",
    "    print(post.prettify())\n",
    "\n",
    "    print('-- '*10)  # для красоты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на семинар 1\n",
    "\n",
    "А что если мы хотим зайти еще глубже по дереву тегов и, например, для каждого заголовка поста найти никнейм юзера, который написал этот пост, и время написания поста?\n",
    "\n",
    "Для этого надо снова зайти в просмотор кода страницы и увидеть, что там просиходит что-то такое:\n",
    "\n",
    "(Заодно обратите внимание, как пишутся комменты в html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<li class=\"content-list__item content-list__item_post shortcuts_item\" id=\"post_474790\">  <!--У каждого поста есть номер-->\n",
    "                \n",
    "    <article class=\"post post_preview\" lang=\"ru\">\n",
    "    <header class=\"post__meta\">\n",
    "        <a href=\"https://habr.com/ru/users/SStrelkov/\" class=\"post__user-info user-info\" title=\"Автор публикации\">\n",
    "            <img src=\"//habrastorage.org/getpro/habr/avatars/62a/d56/1fd/62ad561fd1e297066e0ccdcce8001192.jpg\" width=\"24\" height=\"24\" class=\"user-info__image-pic user-info__image-pic_small\">\n",
    "            <span class=\"user-info__nickname user-info__nickname_small\">SStrelkov</span>  <!--А вот и никнейм-->\n",
    "        </a>                            <!--Никнейм лежит глубже номера, в header, в span class user info...-->\n",
    "\n",
    "        <span class=\"post__time\">сегодня в 10:03</span>  <!--А вот и время, тоже в header, но у span другой класс-->\n",
    "    </header>\n",
    "\n",
    "    <h2 class=\"post__title\">\n",
    "        <a href=\"https://habr.com/ru/company/croc/blog/474790/\" class=\"post__title_link\">Байки переговорщика</a>\n",
    "    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать словарик, где ключом будет id поста, а значением словарь или кортеж из трех переменных: название поста, никнейм автора и время написания для всех постов с главной страницы хабра, на которую мы смотрим весь семинар."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание на семинар 2\n",
    "\n",
    "1. Скачать главную страницу Яндекс.Погоды и <br>\n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;а) распечатать сегодняшнюю температуру и облачность<br>\n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;б) распечатать время восхода и заката<br>\n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;в) погоду на завтра<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хорошая статья про это все\n",
    "\n",
    "[https://sysblok.ru/courses/obkachka-sajtov-svoimi-rukami-razbiraemsja-s-html/](https://sysblok.ru/courses/obkachka-sajtov-svoimi-rukami-razbiraemsja-s-html/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
